networks:
  sentiric-net:
    driver: bridge
    ipam:
      config:
        - subnet: ${NETWORK_SUBNET:-10.88.0.0/16}
          gateway: ${NETWORK_GATEWAY:-10.88.0.1}

volumes:
  # Modelleri tekrar tekrar indirmemek i√ßin kalƒ±cƒ± volume
  sentiric-models:

services:
  # üß† KATMAN 3: LLM GATEWAY SERVICE (RUST)
  llm-gateway-service:
    image: ghcr.io/sentiric/sentiric-llm-gateway-service:latest
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - ../sentiric-certificates:/sentiric-certificates:ro
    environment:
      - ENV=development
      - RUST_LOG=info,sentiric_llm_gateway_service=debug
      
      # KRƒ∞Tƒ∞K D√úZELTME: http:// -> https://
      # Tonic k√ºt√ºphanesi TLS handshake'i ba≈ülatmak i√ßin https ≈üemasƒ± bekler.
      - LLM_LLAMA_SERVICE_GRPC_URL=https://llm-llama-service:16071
      
      # --- G√ºvenlik ---
      - GRPC_TLS_CA_PATH=/sentiric-certificates/certs/ca.crt
      - LLM_GATEWAY_SERVICE_CERT_PATH=/sentiric-certificates/certs/llm-gateway-service.crt
      - LLM_GATEWAY_SERVICE_KEY_PATH=/sentiric-certificates/certs/llm-gateway-service.key
      
    ports:
      - "16020:16020" # HTTP
      - "16021:16021" # gRPC
      - "16022:16022" # Metrics
    networks:
      sentiric-net:
        ipv4_address: ${LLM_GATEWAY_SERVICE_IPV4_ADDRESS:-10.88.60.2}
    restart: always
    depends_on:
      llm-llama-service:
        condition: service_healthy

  # ü¶æ KATMAN 4: LLM LLAMA SERVICE (C++ REAL ENGINE)
  llm-llama-service:
    image: ghcr.io/sentiric/sentiric-llm-llama-service:latest-gpu
    build:
      # Yan klas√∂rdeki ger√ßek projeyi derler
      context: ../sentiric-llm-llama-service
      # GPU varsa 'Dockerfile.gpu' kullanƒ±n, yoksa 'Dockerfile'
      dockerfile: Dockerfile 
    volumes:
      - ../sentiric-certificates:/sentiric-certificates:ro
      - sentiric-models:/models
    environment:
      # --- Performans Ayarlarƒ± ---
      - LLM_LLAMA_SERVICE_GPU_LAYERS=0 # CPU i√ßin 0, GPU i√ßin 99
      - LLM_LLAMA_SERVICE_CONTEXT_SIZE=4096
      - LLM_LLAMA_SERVICE_THREADS=4
      
      # --- Model Ayarlarƒ± (Otomatik ƒ∞ndirir) ---
      # K√º√ß√ºk ve hƒ±zlƒ± bir model se√ßiyoruz (Test i√ßin)
      - LLM_LLAMA_SERVICE_MODEL_ID=Qwen/Qwen2.5-1.5B-Instruct-GGUF
      - LLM_LLAMA_SERVICE_MODEL_FILENAME=qwen2.5-1.5b-instruct-q4_k_m.gguf
      
      # --- Network ---
      - LLM_LLAMA_SERVICE_GRPC_PORT=16071
      - LLM_LLAMA_SERVICE_HTTP_PORT=16070
      
      # --- G√ºvenlik ---
      - GRPC_TLS_CA_PATH=/sentiric-certificates/certs/ca.crt
      - LLM_LLAMA_SERVICE_CERT_PATH=/sentiric-certificates/certs/llm-llama-service.crt
      - LLM_LLAMA_SERVICE_KEY_PATH=/sentiric-certificates/certs/llm-llama-service.key

    ports:
      - "16070:16070" # Engine HTTP
      - "16071:16071" # Engine gRPC
    networks:
      sentiric-net:
        ipv4_address: 10.88.60.7
    healthcheck:
      # Model inene kadar 'unhealthy' d√∂ner, Gateway bunu bekler
      test: ["CMD", "curl", "-f", "http://localhost:16070/health"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60m # Model indirme ve d√∂n√º≈üt√ºrme uzun s√ºrebilir
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]