name: sentiric

x-ai-service-template: &ai-service-template
  restart: always
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [ gpu, compute, utility ] # Genişletildi
  healthcheck:
    interval: 30s
    timeout: 10s
    retries: 10
    start_period: 60m

networks:
  sentiric-net:
    driver: bridge
    name: sentiric-net
    ipam:
      config:
        - subnet: ${NETWORK_SUBNET:-10.88.0.0/16}
          gateway: ${NETWORK_GATEWAY:-10.88.0.1}

volumes:
  llm-llama-models:
  llm-llama-cache:
  llm-llama-loras:

services:
  # =============================================================
  # KATMAN 3: AI Gateway
  # =============================================================
  llm-gateway-service:
    image: ghcr.io/sentiric/sentiric-llm-gateway-service:latest
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - "${CERTIFICATES_REPO_PATH:-../sentiric-certificates}:/sentiric-certificates:ro"
    environment:
      # --- Ağ & Config ---
      - ENV=production
      - RUST_LOG=info,sentiric_llm_gateway=debug
      - HOST=0.0.0.0
      - GRPC_PORT=${LLM_GATEWAY_SERVICE_GRPC_PORT:-16021}
      
      # --- Hedef Servis (Llama) ---
      # Gateway buraya HTTPS ile gidecek, sertifikalar aşağıda tanımlı
      - LLM_LLAMA_SERVICE_GRPC_URL=https://llm-llama-service:${LLM_LLAMA_SERVICE_GRPC_PORT:-16071}
      
      # --- Güvenlik (mTLS Client & Server) ---
      - GRPC_TLS_CA_PATH=/sentiric-certificates/certs/ca.crt
      - LLM_GATEWAY_SERVICE_CERT_PATH=/sentiric-certificates/certs/llm-gateway-service.crt
      - LLM_GATEWAY_SERVICE_KEY_PATH=/sentiric-certificates/certs/llm-gateway-service.key
    networks:
      sentiric-net:
        ipv4_address: ${LLM_GATEWAY_SERVICE_IPV4_ADDRESS:-10.88.60.2}
    ports:
      - "${LLM_GATEWAY_SERVICE_HTTP_PORT:-16020}:${LLM_GATEWAY_SERVICE_HTTP_PORT:-16020}"
      - "${LLM_GATEWAY_SERVICE_GRPC_PORT:-16021}:${LLM_GATEWAY_SERVICE_GRPC_PORT:-16021}"
      - "${LLM_GATEWAY_SERVICE_METRICS_PORT:-16022}:${LLM_GATEWAY_SERVICE_METRICS_PORT:-16022}"
    restart: always
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:${LLM_GATEWAY_SERVICE_HTTP_PORT:-16020}/health" ]
      interval: 30s
      timeout: 5s
      retries: 60
    depends_on:
      llm-llama-service:
        condition: service_healthy

  # =============================================================
  # KATMAN 4: AI SERVİSLERİ
  # =============================================================
  llm-llama-service:
    <<: *ai-service-template
    # GPU Image'ı zorla (Build context yerine image kullanıyoruz ki local CPU build çakışmasın)
    image: ghcr.io/sentiric/sentiric-llm-llama-service:latest-gpu
    volumes:
      - "${CERTIFICATES_REPO_PATH:-../sentiric-certificates}:/sentiric-certificates:ro"
      - llm-llama-models:/models
      - llm-llama-loras:/lora_adapters
    environment:
      # --- GPU & Performans ---
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - LLM_LLAMA_SERVICE_GPU_LAYERS=${LLM_LLAMA_SERVICE_GPU_LAYERS:-100} # Max Offload
      - LLM_LLAMA_SERVICE_CONTEXT_SIZE=${LLM_LLAMA_SERVICE_CONTEXT_SIZE:-4096}
      
      # --- Güvenlik (mTLS Server) ---
      # KRİTİK DÜZELTME: Bu değişkenler eksikti, o yüzden Llama insecure açılıyordu.
      - GRPC_TLS_CA_PATH=/sentiric-certificates/certs/ca.crt
      - LLM_LLAMA_SERVICE_CERT_PATH=/sentiric-certificates/certs/llm-llama-service-chain.crt
      - LLM_LLAMA_SERVICE_KEY_PATH=/sentiric-certificates/certs/llm-llama-service.key
      
      # --- Ağ ---
      - LLM_LLAMA_SERVICE_HOST=0.0.0.0
      - LLM_LLAMA_SERVICE_GRPC_PORT=${LLM_LLAMA_SERVICE_GRPC_PORT:-16071}
      - LLM_LLAMA_SERVICE_HTTP_PORT=${LLM_LLAMA_SERVICE_HTTP_PORT:-16070}
    networks:
      sentiric-net:
        ipv4_address: ${LLM_LLAMA_SERVICE_IPV4_ADDRESS:-10.88.60.7}
    ports:
      - "${LLM_LLAMA_SERVICE_HTTP_PORT:-16070}:${LLM_LLAMA_SERVICE_HTTP_PORT:-16070}"
      - "${LLM_LLAMA_SERVICE_GRPC_PORT:-16071}:${LLM_LLAMA_SERVICE_GRPC_PORT:-16071}"
    healthcheck:
      test: [ "CMD", "curl", "-f", "-k", "https://localhost:${LLM_LLAMA_SERVICE_HTTP_PORT:-16070}/health" ]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60m