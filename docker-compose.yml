name: sentiric

x-ai-service-template: &ai-service-template
  restart: always
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [ gpu, compute, utility ]
  healthcheck:
    interval: 30s
    timeout: 10s
    retries: 10
    start_period: 60m

networks:
  sentiric-net:
    driver: bridge
    name: sentiric-net
    ipam:
      config:
        - subnet: ${NETWORK_SUBNET:-10.88.0.0/16}
          gateway: ${NETWORK_GATEWAY:-10.88.0.1}

volumes:
  llm-llama-models:
  llm-llama-cache:
  llm-llama-loras:

services:
  # =============================================================
  # KATMAN 3: AI Gateway (Rust)
  # =============================================================
  llm-gateway-service:
    image: ghcr.io/sentiric/sentiric-llm-gateway-service:latest
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - "${CERTIFICATES_REPO_PATH:-../sentiric-certificates}:/sentiric-certificates:ro"
    environment:
      # --- Servis Kimliği (Standardize) ---
      - LLM_GATEWAY_SERVICE_HOST=llm-gateway-service
      - LLM_GATEWAY_SERVICE_LISTEN_ADDRESS=0.0.0.0
      - LLM_GATEWAY_SERVICE_HTTP_PORT=16020
      - LLM_GATEWAY_SERVICE_GRPC_PORT=16021
      - LLM_GATEWAY_SERVICE_METRICS_PORT=16022
      
      # --- Config ---
      - ENV=production
      - RUST_LOG=info,sentiric_llm_gateway=debug
      
      # --- Hedef Servis (Llama) ---
      - LLM_LLAMA_SERVICE_GRPC_URL=https://llm-llama-service:16071
      
      # --- Güvenlik (mTLS) - Container İçi Yollar ---
      - GRPC_TLS_CA_PATH=/sentiric-certificates/certs/ca.crt
      - LLM_GATEWAY_SERVICE_CERT_PATH=/sentiric-certificates/certs/llm-gateway-service.crt
      - LLM_GATEWAY_SERVICE_KEY_PATH=/sentiric-certificates/certs/llm-gateway-service.key
    networks:
      sentiric-net:
        ipv4_address: 10.88.60.2
    ports:
      - "16020:16020"
      - "16021:16021"
      - "16022:16022"
    restart: always
    # DÜZELTME: Rust servisi sadece gRPC (16021) dinliyor. HTTP yok.
    # Bu yüzden 'nc' (netcat) ile TCP port kontrolü yapıyoruz.
    healthcheck:
      test: [ "CMD", "nc", "-z", "localhost", "16021" ]
      interval: 10s
      timeout: 5s
      retries: 3
    depends_on:
      llm-llama-service:
        condition: service_healthy

  # =============================================================
  # KATMAN 4: AI SERVİSLERİ (C++ / CUDA)
  # =============================================================
  llm-llama-service:
    <<: *ai-service-template
    image: ghcr.io/sentiric/sentiric-llm-llama-service:latest-gpu
    volumes:
      - "${CERTIFICATES_REPO_PATH:-../sentiric-certificates}:/sentiric-certificates:ro"
      - llm-llama-models:/models
      - llm-llama-loras:/lora_adapters
    environment:
      # --- Servis Kimliği ---
      - LLM_LLAMA_SERVICE_HOST=llm-llama-service
      - LLM_LLAMA_SERVICE_LISTEN_ADDRESS=0.0.0.0
      - LLM_LLAMA_SERVICE_HTTP_PORT=16070
      - LLM_LLAMA_SERVICE_GRPC_PORT=16071
      - LLM_LLAMA_SERVICE_METRICS_PORT=16072
      
      # --- GPU & Performans ---
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - LLM_LLAMA_SERVICE_GPU_LAYERS=100
      - LLM_LLAMA_SERVICE_CONTEXT_SIZE=4096
      - LLM_LLAMA_SERVICE_ENABLE_WARM_UP=true
      
      # --- Güvenlik (mTLS) ---
      - GRPC_TLS_CA_PATH=/sentiric-certificates/certs/ca.crt
      - LLM_LLAMA_SERVICE_CERT_PATH=/sentiric-certificates/certs/llm-llama-service-chain.crt
      - LLM_LLAMA_SERVICE_KEY_PATH=/sentiric-certificates/certs/llm-llama-service.key
    networks:
      sentiric-net:
        ipv4_address: 10.88.60.7
    ports:
      - "16070:16070"
      - "16071:16071"
    # Llama servisi HTTP portunu dinlediği için bu doğrudur
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:16070/health" ]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60m